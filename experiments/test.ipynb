{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_id</th>\n",
       "      <th>content</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>final_sentiment</th>\n",
       "      <th>labels</th>\n",
       "      <th>quadruplet</th>\n",
       "      <th>spam</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>model_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.640000e+18</td>\n",
       "      <td>@tokopedia min aku udah bayar tapi kenapa diba...</td>\n",
       "      <td>min aku udah bayar tapi kenapa dibatalin pesan...</td>\n",
       "      <td>negative</td>\n",
       "      <td>payment; produk;</td>\n",
       "      <td>(pesananku, udah bayar tapi kenapa dibatalin, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>(pesananku, sudah bayar tapi kenapa dibatalin,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    original_id                                            content  \\\n",
       "0  1.640000e+18  @tokopedia min aku udah bayar tapi kenapa diba...   \n",
       "\n",
       "                                         clean_tweet final_sentiment  \\\n",
       "0  min aku udah bayar tapi kenapa dibatalin pesan...        negative   \n",
       "\n",
       "             labels                                         quadruplet spam  \\\n",
       "0  payment; produk;  (pesananku, udah bayar tapi kenapa dibatalin, ...  NaN   \n",
       "\n",
       "  sentiment_label                                   model_prediction  \n",
       "0       sentiment  (pesananku, sudah bayar tapi kenapa dibatalin,...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/quadruplet/model_inference_300_data.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_quadruplet(sequence):\n",
    "    extractions = []\n",
    "    # find all matching quadruplet with (); pattern\n",
    "    quadruplets = re.findall(\"\\(.*?\\)\", sequence)\n",
    "    for quadruplet in quadruplets:\n",
    "        # Remove the in the start \"(\"  and at the end \")\".\n",
    "        quadruplet = quadruplet[1:-1]\n",
    "        try:\n",
    "            aspect_term, opinion_term, sentiment, aspect_category = quadruplet.split(', ')\n",
    "        except ValueError:\n",
    "            aspect_term, opinion_term, sentiment, aspect_category = '', '', '', ''\n",
    "        aspect_term = aspect_term.strip().lower()\n",
    "        opinion_term = opinion_term.strip().lower()\n",
    "        sentiment = sentiment.strip().lower()\n",
    "        aspect_category = aspect_category.strip().lower()\n",
    "        extractions.append((aspect_term, opinion_term, sentiment, aspect_category)) \n",
    "    return extractions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        # == Metrics ==\n",
    "        self.precision_fn = lambda n_tp, n_pred: float(n_tp) / float(n_pred) if n_pred != 0 else 0\n",
    "        self.recall_fn = lambda n_tp, n_gold: float(n_tp) / float(n_gold) if n_gold != 0 else 0\n",
    "        self.f1_fn = (\n",
    "            lambda precision, recall: (2 * precision * recall) / (precision + recall)\n",
    "            if precision != 0 or recall != 0\n",
    "            else 0\n",
    "        )\n",
    "    def score(self, pred, gold):\n",
    "        assert len(pred) == len(gold)\n",
    "        n_tp, n_gold, n_pred = 0, 0, 0\n",
    "\n",
    "        for i in range(len(pred)):\n",
    "            n_gold += len(gold[i])\n",
    "            n_pred += len(pred[i])\n",
    "\n",
    "            for t in pred[i]:\n",
    "                if t in gold[i]:\n",
    "                    n_tp += 1\n",
    "                \n",
    "\n",
    "        precision = self.precision_fn(n_tp, n_pred)\n",
    "        recall = self.recall_fn(n_tp, n_gold)\n",
    "        f1 = self.f1_fn(precision, recall)\n",
    "        return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "    \n",
    "    # == Evaluation ==\n",
    "    def evaluate(self, pred_seqs, gold_seqs):\n",
    "        assert len(pred_seqs) == len(gold_seqs)\n",
    "        num_samples = len(gold_seqs)\n",
    "\n",
    "        all_labels, all_preds = [], []\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            gold_list = extract_quadruplet(gold_seqs[i])\n",
    "            pred_list = extract_quadruplet(pred_seqs[i])\n",
    "\n",
    "            all_labels.append(gold_list)\n",
    "            all_preds.append(pred_list)\n",
    "\n",
    "        raw_scores = self.score(all_preds, all_labels)\n",
    "        return raw_scores, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "raw_scores, all_labels, all_preds = evaluator.evaluate(df['model_prediction'].astype('str'), df['quadruplet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.3333333333333333,\n",
       " 'recall': 0.3333333333333333,\n",
       " 'f1': 0.3333333333333333}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danendra\\anaconda3\\envs\\tesis\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import preprocessor as p\n",
    "from src.slang_word import SLANG_WORDS\n",
    "#gk kepake aslinya, cuma untuk testing aja\n",
    "from transformers import (\n",
    "    AutoTokenizer\n",
    ")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \"\"\"\n",
    "    Untuk preprocess sebelum masuk ke fine tuner\n",
    "    input :\n",
    "        preprocess_type : p0/p1/p2/p3\n",
    "        tokenizer : huggingface tokenizer\n",
    "        tokenizer_max_length : max length hasil tokenizer nantinya\n",
    "        text_col : kolom text raw yang ingin dibersihkan\n",
    "        label_col : kolom label yang ingin diprediksi\n",
    "    out : \n",
    "        tokenized_inputs : huggingface dataset hasil pembersihan dan tokenizer\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 preprocess_type:str , \n",
    "                 tokenizer, \n",
    "                 tokenizer_max_length:int, \n",
    "                 text_col:str,\n",
    "                 label_col:str\n",
    "        ):\n",
    "        self.prepocess_type = preprocess_type\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_max_length = tokenizer_max_length\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        if self.prepocess_type=='p01' or self.prepocess_type=='p03':\n",
    "            #create stopword remover\n",
    "            self.stop_factory = StopWordRemoverFactory()\n",
    "            self.stopword_remover = self.stop_factory.create_stop_word_remover()\n",
    "        if self.prepocess_type=='p02' or self.prepocess_type=='p03':\n",
    "            # create stemmer\n",
    "            self.factory = StemmerFactory()\n",
    "            self.stemmer = self.factory.create_stemmer()\n",
    "        \n",
    "    def clean_repetitive(self, word):\n",
    "        prev_char = None\n",
    "        char_count=-1\n",
    "        clean_word=''\n",
    "        for c in word:\n",
    "            if prev_char!=c:\n",
    "                prev_char=c\n",
    "                char_count=0\n",
    "            else:\n",
    "                char_count+=1\n",
    "            if char_count<1:\n",
    "                clean_word+=c\n",
    "        #remove word if only 1 char left\n",
    "        return clean_word if len(clean_word)>1 else ''\n",
    "    def clean_text(self, text):\n",
    "        #lower case\n",
    "        text = text.lower()\n",
    "        #clean text with tweet-preprocessor\n",
    "        text = p.clean(text)\n",
    "        #clean repetitive word\n",
    "        text = \" \".join([self.clean_repetitive(word) for word in text.split()])\n",
    "        #convert slang word into dictionary\n",
    "        text = \" \".join([SLANG_WORDS[word] if word in SLANG_WORDS else word for word in text.split()])\n",
    "        return text\n",
    "    def stem(self, text):\n",
    "        return self.stemmer.stem(text)\n",
    "    def stopword_removal(self, text):\n",
    "        return self.stopword_remover.remove(text)\n",
    "    def preprocess_dataset(self, examples):\n",
    "        inputs = examples[self.text_col]\n",
    "        inputs = [self.clean_text(input) for input in inputs]\n",
    "        if self.prepocess_type=='p02' or self.prepocess_type=='p04':\n",
    "            inputs = [self.stopword_removal(input) for input in inputs]\n",
    "        if self.prepocess_type=='p03' or self.prepocess_type=='p04':\n",
    "            inputs = [self.stem(input) for input in inputs]\n",
    "        targets =examples[self.label_col] \n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            inputs, text_target=targets, max_length=self.tokenizer_max_length, truncation=True\n",
    "        )\n",
    "        return tokenized_inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained_checkpoint = \"Wikidepia/IndoT5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_pretrained_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "text_col = 'content'\n",
    "label_col = 'quadruplet'\n",
    "preprocessor = Preprocessor('p00', tokenizer, max_length, text_col, label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/danendra/.cache/huggingface/datasets/csv/default-bf79bf034d0392b1/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "Loading cached split indices for dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-bf79bf034d0392b1\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-571d28bc462ba89a.arrow and C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-bf79bf034d0392b1\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-24355355d1083472.arrow\n",
      "                                                   \r"
     ]
    }
   ],
   "source": [
    "#dataset\n",
    "raw_dataset = load_dataset('csv', data_files='../Data/quadruplet/quadruplet_only.csv')\n",
    "splitted_dataset = raw_dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "tokenized_dataset = splitted_dataset.map(preprocessor.preprocess_dataset, batched=True, remove_columns=splitted_dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(pesananku, udah bayar tapi kenapa dibatalin, negative, website&apps);</s>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_dataset['test']['labels'][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuner:\n",
    "    def __init__(self, model, save_path, tokenizer, train_dataset, eval_dataset) -> None:\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = model.to(self.device)\n",
    "        self.save_path = save_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "        print(self.device)\n",
    "    def fine_tune(self, arg):\n",
    "        data_collator = DataCollatorForSeq2Seq(self.tokenizer, model=self.model)\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            self.model,\n",
    "            arg,\n",
    "            train_dataset=self.train_dataset,\n",
    "            eval_dataset=self.eval_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "        trainer.train()\n",
    "        self.model.save_pretrained(self.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danendra\\anaconda3\\envs\\tesis\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/195 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "                                                \n",
      " 20%|██        | 39/195 [00:09<00:29,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2513034343719482, 'eval_runtime': 0.4241, 'eval_samples_per_second': 181.551, 'eval_steps_per_second': 23.578, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 40%|████      | 78/195 [00:40<00:22,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5867531299591064, 'eval_runtime': 0.4142, 'eval_samples_per_second': 185.921, 'eval_steps_per_second': 24.146, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|██████    | 117/195 [01:15<00:12,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3198323249816895, 'eval_runtime': 0.3976, 'eval_samples_per_second': 193.646, 'eval_steps_per_second': 25.149, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|████████  | 156/195 [01:49<00:06,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.185319185256958, 'eval_runtime': 0.3817, 'eval_samples_per_second': 201.746, 'eval_steps_per_second': 26.201, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 195/195 [02:23<00:00,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.147050619125366, 'eval_runtime': 0.4064, 'eval_samples_per_second': 189.472, 'eval_steps_per_second': 24.607, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [02:53<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 173.2776, 'train_samples_per_second': 8.887, 'train_steps_per_second': 1.125, 'train_loss': 2.9481980543870194, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "#constant\n",
    "save_path = f'../models/test_fine_tuner'\n",
    "model_pretrained_checkpoint = \"Wikidepia/IndoT5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_pretrained_checkpoint)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_pretrained_checkpoint)\n",
    "#training argument\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    save_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    resume_from_checkpoint=True,\n",
    "    num_train_epochs=5,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "finetuner = FineTuner(model=model, save_path=save_path, tokenizer=tokenizer, \n",
    "                      train_dataset=tokenized_dataset['train'], eval_dataset=tokenized_dataset['test'])\n",
    "finetuner.fine_tune(training_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInference:\n",
    "    def __init__(self, batch_size, dataset, model, tokenizer) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self.model = model\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.datacollator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "    def inference(self):\n",
    "        print(len(self.dataset['input_ids']))\n",
    "        inference_dataset = [self.dataset[i] for i in range(len(self.dataset['input_ids']))]\n",
    "        inference_dataset = self.datacollator(inference_dataset)\n",
    "        pred_text = []\n",
    "        for i in tqdm(range(0, len(inference_dataset['input_ids']), self.batch_size)):\n",
    "            generated_text = model.generate(inference_dataset['input_ids'][i:i+self.batch_size].to('cuda'), max_length=100)\n",
    "            decoded_text = self.tokenizer.batch_decode(generated_text, skip_special_tokens=True)\n",
    "            pred_text+=decoded_text\n",
    "        return pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constant\n",
    "save_path = f'../models/pt-indot5'\n",
    "model = T5ForConditionalGeneration.from_pretrained(save_path).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "model_inference = ModelInference(batch_size=8, dataset=tokenized_dataset['test'], model=model, tokenizer=tokenizer)\n",
    "pred_text = model_inference.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-bf79bf034d0392b1\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-37557ea6cb933a33.arrow\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 142.94ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37874"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = splitted_dataset['test']\n",
    "test_dataset = test_dataset.add_column(f'model_prediction', pred_text)\n",
    "test_dataset.to_csv('../Data/quadruplet/model_inference_300_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pipeline dengan data sedikit\n",
    "pakai data yg cuma 300an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danendra\\anaconda3\\envs\\tesis\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.finetuner import FineTuner\n",
    "from src.preprocessor import Preprocessor\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constant\n",
    "max_length = 128\n",
    "text_col = 'content'\n",
    "label_col = 'quadruplet'\n",
    "preprocess_type = 'p00'\n",
    "SAVE_PATH = f'../models/test_fine_tuner_with_300_data'\n",
    "PRETRAINED_MODEL = \"Wikidepia/IndoT5-base\"\n",
    "DATA_PATH = '../Data/quadruplet/quadruplet_only.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "preprocessor = Preprocessor(preprocess_type, tokenizer, max_length, text_col, label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/danendra/.cache/huggingface/datasets/csv/default-bf79bf034d0392b1/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 1/1 [00:00<00:00, 677.48it/s]\n",
      "Loading cached split indices for dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-bf79bf034d0392b1\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-571d28bc462ba89a.arrow and C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-bf79bf034d0392b1\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-24355355d1083472.arrow\n",
      "Loading cached processed dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-bf79bf034d0392b1\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-6635831735983f73.arrow\n",
      "Loading cached processed dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-bf79bf034d0392b1\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-bcf14b21438c2d33.arrow\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset('csv', data_files=DATA_PATH)\n",
    "splitted_dataset = raw_dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "tokenized_dataset = splitted_dataset.map(preprocessor.preprocess_dataset, batched=True, remove_columns=splitted_dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 308\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 77\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@tokopedia min aku udah bayar tapi kenapa dibatalin pesanan ku?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_dataset['test']['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'min aku sudah bayar tapi kenapa dibatalin pesanan ku?</s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_dataset['test']['input_ids'][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training argument\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    SAVE_PATH,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    resume_from_checkpoint=True,\n",
    "    num_train_epochs=100,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "finetuner = FineTuner(model=model, save_path=SAVE_PATH, tokenizer=tokenizer, \n",
    "                      train_dataset=tokenized_dataset['train'], eval_dataset=tokenized_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danendra\\anaconda3\\envs\\tesis\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/3900 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "                                                  \n",
      "  1%|          | 39/3900 [00:09<12:09,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.16196346282959, 'eval_runtime': 0.4602, 'eval_samples_per_second': 167.325, 'eval_steps_per_second': 21.73, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "  2%|▏         | 78/3900 [00:40<10:59,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.395500659942627, 'eval_runtime': 0.4113, 'eval_samples_per_second': 187.215, 'eval_steps_per_second': 24.314, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "  3%|▎         | 117/3900 [01:10<11:20,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.009577512741089, 'eval_runtime': 0.4159, 'eval_samples_per_second': 185.134, 'eval_steps_per_second': 24.043, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  4%|▍         | 156/3900 [01:40<10:52,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7476584911346436, 'eval_runtime': 0.398, 'eval_samples_per_second': 193.452, 'eval_steps_per_second': 25.124, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  5%|▌         | 195/3900 [02:14<11:21,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6053522825241089, 'eval_runtime': 0.4155, 'eval_samples_per_second': 185.309, 'eval_steps_per_second': 24.066, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  6%|▌         | 234/3900 [02:46<11:03,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5093351602554321, 'eval_runtime': 0.4228, 'eval_samples_per_second': 182.139, 'eval_steps_per_second': 23.654, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  7%|▋         | 273/3900 [03:15<10:49,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.451578140258789, 'eval_runtime': 0.399, 'eval_samples_per_second': 193.0, 'eval_steps_per_second': 25.065, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  8%|▊         | 312/3900 [03:49<10:14,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.394089698791504, 'eval_runtime': 0.4039, 'eval_samples_per_second': 190.627, 'eval_steps_per_second': 24.757, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  9%|▉         | 351/3900 [04:19<10:16,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.341862678527832, 'eval_runtime': 0.414, 'eval_samples_per_second': 186.005, 'eval_steps_per_second': 24.157, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 10%|█         | 390/3900 [04:50<11:14,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3095558881759644, 'eval_runtime': 0.4258, 'eval_samples_per_second': 180.833, 'eval_steps_per_second': 23.485, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 11%|█         | 429/3900 [05:24<10:54,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2794133424758911, 'eval_runtime': 0.409, 'eval_samples_per_second': 188.245, 'eval_steps_per_second': 24.447, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 12%|█▏        | 468/3900 [05:55<10:37,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.244598388671875, 'eval_runtime': 0.4159, 'eval_samples_per_second': 185.126, 'eval_steps_per_second': 24.042, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 501/3900 [06:22<12:40,  4.47it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8139, 'learning_rate': 1.7435897435897438e-05, 'epoch': 12.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 13%|█▎        | 507/3900 [06:24<10:27,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2117669582366943, 'eval_runtime': 0.4101, 'eval_samples_per_second': 187.775, 'eval_steps_per_second': 24.386, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 14%|█▍        | 546/3900 [06:56<09:42,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1925514936447144, 'eval_runtime': 0.411, 'eval_samples_per_second': 187.361, 'eval_steps_per_second': 24.333, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 15%|█▌        | 585/3900 [07:25<09:16,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1650327444076538, 'eval_runtime': 0.408, 'eval_samples_per_second': 188.703, 'eval_steps_per_second': 24.507, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 16%|█▌        | 624/3900 [07:56<10:06,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1526316404342651, 'eval_runtime': 0.4189, 'eval_samples_per_second': 183.83, 'eval_steps_per_second': 23.874, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 17%|█▋        | 663/3900 [08:28<09:19,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1317026615142822, 'eval_runtime': 0.4245, 'eval_samples_per_second': 181.37, 'eval_steps_per_second': 23.555, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 18%|█▊        | 702/3900 [08:57<09:13,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1117113828659058, 'eval_runtime': 0.4079, 'eval_samples_per_second': 188.767, 'eval_steps_per_second': 24.515, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 19%|█▉        | 741/3900 [09:27<09:27,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0954102277755737, 'eval_runtime': 0.4609, 'eval_samples_per_second': 167.078, 'eval_steps_per_second': 21.698, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 20%|██        | 780/3900 [09:57<08:58,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0710582733154297, 'eval_runtime': 0.405, 'eval_samples_per_second': 190.145, 'eval_steps_per_second': 24.694, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 21%|██        | 819/3900 [10:28<09:21,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0458658933639526, 'eval_runtime': 0.4134, 'eval_samples_per_second': 186.245, 'eval_steps_per_second': 24.188, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 22%|██▏       | 858/3900 [10:59<09:46,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0285292863845825, 'eval_runtime': 0.4139, 'eval_samples_per_second': 186.033, 'eval_steps_per_second': 24.16, 'epoch': 22.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 23%|██▎       | 897/3900 [11:30<10:45,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0133144855499268, 'eval_runtime': 0.4114, 'eval_samples_per_second': 187.157, 'eval_steps_per_second': 24.306, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 24%|██▍       | 936/3900 [12:01<08:30,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9914776682853699, 'eval_runtime': 0.4048, 'eval_samples_per_second': 190.199, 'eval_steps_per_second': 24.701, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 25%|██▌       | 975/3900 [12:30<08:07,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9681766033172607, 'eval_runtime': 0.4067, 'eval_samples_per_second': 189.336, 'eval_steps_per_second': 24.589, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 1001/3900 [13:00<09:26,  5.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.778, 'learning_rate': 1.4871794871794874e-05, 'epoch': 25.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 26%|██▌       | 1014/3900 [13:03<08:23,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9572198987007141, 'eval_runtime': 0.4066, 'eval_samples_per_second': 189.398, 'eval_steps_per_second': 24.597, 'epoch': 26.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 27%|██▋       | 1053/3900 [13:34<07:57,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9366672039031982, 'eval_runtime': 0.4081, 'eval_samples_per_second': 188.659, 'eval_steps_per_second': 24.501, 'epoch': 27.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 28%|██▊       | 1092/3900 [14:04<08:31,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9187056422233582, 'eval_runtime': 0.4222, 'eval_samples_per_second': 182.398, 'eval_steps_per_second': 23.688, 'epoch': 28.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 29%|██▉       | 1131/3900 [14:35<09:07,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8971647620201111, 'eval_runtime': 0.477, 'eval_samples_per_second': 161.424, 'eval_steps_per_second': 20.964, 'epoch': 29.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 30%|███       | 1170/3900 [15:06<07:54,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8863543272018433, 'eval_runtime': 0.3908, 'eval_samples_per_second': 197.037, 'eval_steps_per_second': 25.589, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 31%|███       | 1209/3900 [15:35<07:50,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8811765313148499, 'eval_runtime': 0.4054, 'eval_samples_per_second': 189.931, 'eval_steps_per_second': 24.666, 'epoch': 31.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 32%|███▏      | 1248/3900 [16:05<08:01,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8534305095672607, 'eval_runtime': 0.3997, 'eval_samples_per_second': 192.666, 'eval_steps_per_second': 25.022, 'epoch': 32.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 33%|███▎      | 1287/3900 [16:36<07:42,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8523573279380798, 'eval_runtime': 0.4093, 'eval_samples_per_second': 188.14, 'eval_steps_per_second': 24.434, 'epoch': 33.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 34%|███▍      | 1326/3900 [17:04<07:27,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8454421162605286, 'eval_runtime': 0.3896, 'eval_samples_per_second': 197.623, 'eval_steps_per_second': 25.665, 'epoch': 34.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 35%|███▌      | 1365/3900 [17:35<06:55,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8551997542381287, 'eval_runtime': 0.4136, 'eval_samples_per_second': 186.181, 'eval_steps_per_second': 24.179, 'epoch': 35.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 36%|███▌      | 1404/3900 [18:14<08:20,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8509798645973206, 'eval_runtime': 0.4287, 'eval_samples_per_second': 179.605, 'eval_steps_per_second': 23.325, 'epoch': 36.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 37%|███▋      | 1443/3900 [18:44<07:24,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8410384654998779, 'eval_runtime': 0.4292, 'eval_samples_per_second': 179.414, 'eval_steps_per_second': 23.3, 'epoch': 37.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 38%|███▊      | 1482/3900 [19:16<06:59,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8459141850471497, 'eval_runtime': 0.4052, 'eval_samples_per_second': 190.017, 'eval_steps_per_second': 24.678, 'epoch': 38.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1501/3900 [19:44<08:57,  4.47it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3848, 'learning_rate': 1.230769230769231e-05, 'epoch': 38.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 39%|███▉      | 1521/3900 [19:48<06:58,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.848955512046814, 'eval_runtime': 0.4193, 'eval_samples_per_second': 183.652, 'eval_steps_per_second': 23.851, 'epoch': 39.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 40%|████      | 1560/3900 [20:19<06:39,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8472787141799927, 'eval_runtime': 0.3993, 'eval_samples_per_second': 192.846, 'eval_steps_per_second': 25.045, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 41%|████      | 1599/3900 [20:53<06:40,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8640817403793335, 'eval_runtime': 0.4037, 'eval_samples_per_second': 190.758, 'eval_steps_per_second': 24.774, 'epoch': 41.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 42%|████▏     | 1638/3900 [21:25<06:43,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.872225284576416, 'eval_runtime': 0.4043, 'eval_samples_per_second': 190.454, 'eval_steps_per_second': 24.734, 'epoch': 42.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 43%|████▎     | 1677/3900 [21:55<06:36,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8672143220901489, 'eval_runtime': 0.4521, 'eval_samples_per_second': 170.301, 'eval_steps_per_second': 22.117, 'epoch': 43.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 44%|████▍     | 1716/3900 [22:28<07:53,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8820900917053223, 'eval_runtime': 0.4879, 'eval_samples_per_second': 157.831, 'eval_steps_per_second': 20.498, 'epoch': 44.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 45%|████▌     | 1755/3900 [23:05<06:48,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8892061114311218, 'eval_runtime': 0.4312, 'eval_samples_per_second': 178.574, 'eval_steps_per_second': 23.191, 'epoch': 45.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 46%|████▌     | 1794/3900 [23:34<06:49,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9008430242538452, 'eval_runtime': 0.4127, 'eval_samples_per_second': 186.586, 'eval_steps_per_second': 24.232, 'epoch': 46.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 47%|████▋     | 1833/3900 [24:05<06:46,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9102492928504944, 'eval_runtime': 0.4249, 'eval_samples_per_second': 181.24, 'eval_steps_per_second': 23.538, 'epoch': 47.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 48%|████▊     | 1872/3900 [24:36<05:43,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9198119044303894, 'eval_runtime': 0.4034, 'eval_samples_per_second': 190.867, 'eval_steps_per_second': 24.788, 'epoch': 48.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 49%|████▉     | 1911/3900 [25:10<05:53,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9303978085517883, 'eval_runtime': 0.4053, 'eval_samples_per_second': 190.0, 'eval_steps_per_second': 24.675, 'epoch': 49.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 50%|█████     | 1950/3900 [25:41<05:34,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.922232985496521, 'eval_runtime': 0.4028, 'eval_samples_per_second': 191.144, 'eval_steps_per_second': 24.824, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 51%|█████     | 1989/3900 [26:13<05:52,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9388132691383362, 'eval_runtime': 0.4191, 'eval_samples_per_second': 183.71, 'eval_steps_per_second': 23.858, 'epoch': 51.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 2001/3900 [26:42<11:16,  2.81it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2081, 'learning_rate': 9.743589743589744e-06, 'epoch': 51.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 52%|█████▏    | 2028/3900 [26:47<05:31,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9570145010948181, 'eval_runtime': 0.4013, 'eval_samples_per_second': 191.863, 'eval_steps_per_second': 24.917, 'epoch': 52.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 53%|█████▎    | 2067/3900 [27:19<05:23,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.959748387336731, 'eval_runtime': 0.4058, 'eval_samples_per_second': 189.771, 'eval_steps_per_second': 24.646, 'epoch': 53.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 54%|█████▍    | 2106/3900 [27:50<05:23,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9719874262809753, 'eval_runtime': 0.4025, 'eval_samples_per_second': 191.288, 'eval_steps_per_second': 24.843, 'epoch': 54.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 55%|█████▌    | 2145/3900 [28:26<05:37,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9694761037826538, 'eval_runtime': 0.4258, 'eval_samples_per_second': 180.85, 'eval_steps_per_second': 23.487, 'epoch': 55.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 56%|█████▌    | 2184/3900 [28:59<05:47,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9796651601791382, 'eval_runtime': 0.4595, 'eval_samples_per_second': 167.561, 'eval_steps_per_second': 21.761, 'epoch': 56.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 57%|█████▋    | 2223/3900 [29:32<05:32,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9842897057533264, 'eval_runtime': 0.4498, 'eval_samples_per_second': 171.2, 'eval_steps_per_second': 22.234, 'epoch': 57.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m finetuner\u001b[39m.\u001b[39;49mfine_tune(training_args)\n",
      "File \u001b[1;32md:\\Kuliah\\S2\\Tesis\\Code\\Buatan Sendiri\\experiments\\src\\finetuner.py:26\u001b[0m, in \u001b[0;36mFineTuner.fine_tune\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m     17\u001b[0m data_collator \u001b[39m=\u001b[39m DataCollatorForSeq2Seq(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer, model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n\u001b[0;32m     18\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[0;32m     20\u001b[0m     arg,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     tokenizer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer,\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msave_pretrained(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_path)\n",
      "File \u001b[1;32mc:\\Users\\danendra\\anaconda3\\envs\\tesis\\lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1667\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\danendra\\anaconda3\\envs\\tesis\\lib\\site-packages\\transformers\\trainer.py:2021\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2018\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   2020\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m-> 2021\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   2023\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[0;32m   2024\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[0;32m   2025\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danendra\\anaconda3\\envs\\tesis\\lib\\site-packages\\transformers\\trainer.py:2291\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2288\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2290\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n\u001b[1;32m-> 2291\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_checkpoint(model, trial, metrics\u001b[39m=\u001b[39;49mmetrics)\n\u001b[0;32m   2292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_save(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\danendra\\anaconda3\\envs\\tesis\\lib\\site-packages\\transformers\\trainer.py:2382\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2379\u001b[0m             torch\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mstate_dict(), os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, SCALER_NAME))\n\u001b[0;32m   2380\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed:\n\u001b[0;32m   2381\u001b[0m     \u001b[39m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[1;32m-> 2382\u001b[0m     torch\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstate_dict(), os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(output_dir, OPTIMIZER_NAME))\n\u001b[0;32m   2383\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings(record\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m caught_warnings:\n\u001b[0;32m   2384\u001b[0m         torch\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mstate_dict(), os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, SCHEDULER_NAME))\n",
      "File \u001b[1;32mc:\\Users\\danendra\\anaconda3\\envs\\tesis\\lib\\site-packages\\torch\\serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\danendra\\anaconda3\\envs\\tesis\\lib\\site-packages\\torch\\serialization.py:668\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[39m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m    667\u001b[0m num_bytes \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39mnbytes()\n\u001b[1;32m--> 668\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(name, storage\u001b[39m.\u001b[39mdata_ptr(), num_bytes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "finetuner.fine_tune(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(SAVE_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference import ModelInference\n",
    "from src.preprocessor import Preprocessor\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constant\n",
    "max_length = 128\n",
    "text_col = 'content'\n",
    "label_col = 'quadruplet'\n",
    "preprocess_type = 'p00'\n",
    "SAVE_PATH = f'../models/pt-indot5'\n",
    "PRETRAINED_MODEL = \"Wikidepia/IndoT5-base\"\n",
    "DATA_PATH = '../Data/quadruplet/quadruplet_only.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(SAVE_PATH).to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/danendra/.cache/huggingface/datasets/csv/default-bf79bf034d0392b1/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 1/1 [00:00<00:00, 497.37it/s]\n",
      "Loading cached split indices for dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-bf79bf034d0392b1\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-571d28bc462ba89a.arrow and C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-bf79bf034d0392b1\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-24355355d1083472.arrow\n",
      "Loading cached processed dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-bf79bf034d0392b1\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-6635831735983f73.arrow\n",
      "Loading cached processed dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-bf79bf034d0392b1\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-bcf14b21438c2d33.arrow\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor(preprocess_type, tokenizer, max_length, text_col, label_col)\n",
    "#dataset\n",
    "raw_dataset = load_dataset('csv', data_files=DATA_PATH)\n",
    "splitted_dataset = raw_dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "tokenized_dataset = splitted_dataset.map(preprocessor.preprocess_dataset, batched=True, remove_columns=splitted_dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "model_inference = ModelInference(batch_size=8, dataset=tokenized_dataset['test'], model=model, tokenizer=tokenizer)\n",
    "pred_text = model_inference.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-bf79bf034d0392b1\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-37557ea6cb933a33.arrow\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 139.76ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37878"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = splitted_dataset['test']\n",
    "test_dataset = test_dataset.add_column(f'{preprocess_type}_model_prediction', pred_text)\n",
    "test_dataset.to_csv('../Data/quadruplet/model_inference_300_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.evaluator import Evaluator\n",
    "from src.postprocessor import PostProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_id</th>\n",
       "      <th>content</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>final_sentiment</th>\n",
       "      <th>labels</th>\n",
       "      <th>quadruplet</th>\n",
       "      <th>spam</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>p00_model_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.640000e+18</td>\n",
       "      <td>@tokopedia min aku udah bayar tapi kenapa diba...</td>\n",
       "      <td>min aku udah bayar tapi kenapa dibatalin pesan...</td>\n",
       "      <td>negative</td>\n",
       "      <td>payment; produk;</td>\n",
       "      <td>(pesananku, udah bayar tapi kenapa dibatalin, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>(pesananku, sudah bayar tapi kenapa dibatalin,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.640000e+18</td>\n",
       "      <td>Seperti biasa menghibungi call center @sicepat...</td>\n",
       "      <td>Seperti biasa menghibungi call center hanya be...</td>\n",
       "      <td>negative</td>\n",
       "      <td>delivery; produk;</td>\n",
       "      <td>(call center, hanya berbelit belit, negative, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>(cal center, hanya berbelit belit, negative, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.640000e+18</td>\n",
       "      <td>@tanyakanrl alfa kalo jumat-minggu ada promo t...</td>\n",
       "      <td>alfa kalo jumat-minggu ada promo tuh jd murah ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>price; produk;</td>\n",
       "      <td>(alfa, kalo jumat-minggu ada promo, positive, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>(alfa, kalau jumat-mingu ada promo, positive, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.640000e+18</td>\n",
       "      <td>@tokopedia apakah tokopedia care sedang gangguan?</td>\n",
       "      <td>apakah tokopedia care sedang gangguan?</td>\n",
       "      <td>negative</td>\n",
       "      <td>website&amp;apps; produk;</td>\n",
       "      <td>(tokopedia, gangguan, negative, website&amp;apps);</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>(tokopedia care, sedang ganguan, neutral, webs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.640000e+18</td>\n",
       "      <td>Beli token listrik lewat tokped error, direjec...</td>\n",
       "      <td>Beli token listrik lewat tokped error, direjec...</td>\n",
       "      <td>negative</td>\n",
       "      <td>website&amp;apps; payment; produk;</td>\n",
       "      <td>(token listrik, lewat tokped error, negative, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>(token listrik, lewat tokped eror, negative, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    original_id                                            content  \\\n",
       "0  1.640000e+18  @tokopedia min aku udah bayar tapi kenapa diba...   \n",
       "1  1.640000e+18  Seperti biasa menghibungi call center @sicepat...   \n",
       "2  1.640000e+18  @tanyakanrl alfa kalo jumat-minggu ada promo t...   \n",
       "3  1.640000e+18  @tokopedia apakah tokopedia care sedang gangguan?   \n",
       "4  1.640000e+18  Beli token listrik lewat tokped error, direjec...   \n",
       "\n",
       "                                         clean_tweet final_sentiment  \\\n",
       "0  min aku udah bayar tapi kenapa dibatalin pesan...        negative   \n",
       "1  Seperti biasa menghibungi call center hanya be...        negative   \n",
       "2  alfa kalo jumat-minggu ada promo tuh jd murah ...         neutral   \n",
       "3             apakah tokopedia care sedang gangguan?        negative   \n",
       "4  Beli token listrik lewat tokped error, direjec...        negative   \n",
       "\n",
       "                           labels  \\\n",
       "0                payment; produk;   \n",
       "1               delivery; produk;   \n",
       "2                  price; produk;   \n",
       "3           website&apps; produk;   \n",
       "4  website&apps; payment; produk;   \n",
       "\n",
       "                                          quadruplet spam sentiment_label  \\\n",
       "0  (pesananku, udah bayar tapi kenapa dibatalin, ...  NaN       sentiment   \n",
       "1  (call center, hanya berbelit belit, negative, ...  NaN       sentiment   \n",
       "2  (alfa, kalo jumat-minggu ada promo, positive, ...  NaN       sentiment   \n",
       "3     (tokopedia, gangguan, negative, website&apps);  NaN       sentiment   \n",
       "4  (token listrik, lewat tokped error, negative, ...  NaN       sentiment   \n",
       "\n",
       "                                p00_model_prediction  \n",
       "0  (pesananku, sudah bayar tapi kenapa dibatalin,...  \n",
       "1  (cal center, hanya berbelit belit, negative, c...  \n",
       "2  (alfa, kalau jumat-mingu ada promo, positive, ...  \n",
       "3  (tokopedia care, sedang ganguan, neutral, webs...  \n",
       "4  (token listrik, lewat tokped eror, negative, w...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/quadruplet/model_inference_300_data.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessor = PostProcessor(use_postprocess=False)\n",
    "evaluator = Evaluator(task_type='quadruplet', postprocessor=postprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 76913.89it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_scores, all_labels, all_preds = evaluator.evaluate(pred_seqs=df['p00_model_prediction'],\n",
    "                   gold_seqs=df['quadruplet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.3333333333333333,\n",
       " 'recall': 0.3333333333333333,\n",
       " 'f1': 0.3333333333333333}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('pesananku',\n",
       "   'udah bayar tapi kenapa dibatalin',\n",
       "   'negative',\n",
       "   'website&apps')],\n",
       " [('call center', 'hanya berbelit belit', 'negative', 'customerservice'),\n",
       "  ('transaksi', 'tdk bisa di cancel', 'negative', 'delivery')],\n",
       " [('alfa', 'kalo jumat-minggu ada promo', 'positive', 'price')],\n",
       " [('tokopedia', 'gangguan', 'negative', 'website&apps')],\n",
       " [('token listrik', 'lewat tokped error', 'negative', 'website&apps')],\n",
       " [('default paymentnya', 'tanpa ada konfirmasi ulang', 'negative', 'payment')],\n",
       " [('payday', 'bulan ini gak dapet apa-apa', 'neutral', 'product')],\n",
       " [('tiktok shop', 'dapat harga plg murah', 'positive', 'price')],\n",
       " [('tokopedia', 'makin ke sini makin pelit aja', 'negative', 'price')],\n",
       " [('ghd', 'lebi lancar nyatok rambutnya', 'positive', 'product'),\n",
       "  ('hairbeauron', 'lebih tersendat', 'negative', 'product'),\n",
       "  ('catokan mahal', 'gak bau gosong', 'positive', 'product')],\n",
       " [('tokopedia', 'ongkir kelewat mahal', 'negative', 'delivery'),\n",
       "  ('tokopedia', 'sistem gak aman', 'negative', 'website&apps')],\n",
       " [('bebas ongkirnya', 'cimit banget sih', 'negative', 'delivery')],\n",
       " [('kurir', 'g dateng2', 'negative', 'delivery')],\n",
       " [('tokped', 'selalu dapet panawaran yg murah2', 'positive', 'price'),\n",
       "  ('shopee', 'selalu dapet panawaran yg murah2', 'positive', 'price'),\n",
       "  ('isi pulsa', 'selalu dapet panawaran yg murah2', 'positive', 'price')],\n",
       " [('lazada', 'nombok ongkir', 'negative', 'delivery')],\n",
       " [('tokopedia now',\n",
       "   'nemu toko yg lebih murah dan barangnya lengkap',\n",
       "   'negative',\n",
       "   'product')],\n",
       " [('kurir', 'paket dibawa kabur', 'negative', 'delivery')],\n",
       " [('verifikasi transfer', 'gabisa dipercepat apa', 'negative', 'payment')],\n",
       " [('tokped', 'mksh pernah kasih promo gila gilaan', 'positive', 'price')],\n",
       " [('harga', 'miring', 'positif', 'price')],\n",
       " [('lazada', 'banyak tuh promo atau vouchernya', 'positive', 'price')],\n",
       " [('tiktok shop', 'harganya lebih murah', 'neutral', 'price')],\n",
       " [('akun shopee', 'gak bisa transaksi', 'negative', 'website&apps')],\n",
       " [('seller', 'lelet amat', 'negative', 'customerservice')],\n",
       " [('paylatter shopee',\n",
       "   'semakin gede aja biaya admin nya',\n",
       "   'negative',\n",
       "   'payment')],\n",
       " [('shopee', 'gratis ongkir', 'positive', 'delivery')],\n",
       " [('belanja',\n",
       "   'masih harus repot repot ngurus ini itu ke polisi',\n",
       "   'negative',\n",
       "   'customerservice'),\n",
       "  ('belanja',\n",
       "   'tanggung jawab cuma sebatas berbagi informasi',\n",
       "   'negative',\n",
       "   'customerservice')],\n",
       " [('tokped', 'ongkirnya sekarang malah mehong', 'negative', 'delivery')],\n",
       " [('login tokped', 'bisa dibantu gaa', 'neutral', 'website&apps')],\n",
       " [('pengiriman', 'driver ngeluh', 'neutral', 'delivery')],\n",
       " [('ekspedisi',\n",
       "   'cara kerja kalian sekarang kurang bagus',\n",
       "   'negative',\n",
       "   'delivery')],\n",
       " [('pengiriman nextday',\n",
       "   'terlambat kalau barang diterima h+2',\n",
       "   'neutral',\n",
       "   'delivery')],\n",
       " [('blibli',\n",
       "   'segampang itu pengirimannya area jabodetabek',\n",
       "   'positive',\n",
       "   'delivery'),\n",
       "  ('blibli',\n",
       "   'secepat itu pengirimannya area jabodetabek',\n",
       "   'positive',\n",
       "   'delivery')],\n",
       " [('voucher', 'ga paham caranya', 'neutral', 'price')],\n",
       " [('tokped', 'gercep bgtttt', 'positive', 'delivery'),\n",
       "  ('laadut', 'lelet bgttt', 'negative', 'delivery')],\n",
       " [('tokped', 'gencarnya ongkir naik', 'negative', 'delivery'),\n",
       "  ('lazada', 'gencarnya ongkir naik', 'negative', 'delivery'),\n",
       "  ('shopee',\n",
       "   'malah turun bahkan dibawah sebelum pandemi',\n",
       "   'positive',\n",
       "   'delivery')],\n",
       " [('barang', 'murah', 'positif', 'price')],\n",
       " [('tiket kereta', 'lg promo jd menggiurkan', 'positive', 'price')],\n",
       " [('the ordinary', 'sering diskon', 'positive', 'price')],\n",
       " [('livin', 'lagi gangguan kah', 'neutral', 'website&apps')],\n",
       " [('tokped', 'lama banget loading mulu', 'negative', 'website&apps')],\n",
       " [('tokopedia', 'ongkirnya gak ngotak lg sekarang', 'negative', 'delivery')],\n",
       " [('gopaylater',\n",
       "   'apakah bisa saya bayar seelum tgl ditiap bulannya',\n",
       "   'neutral',\n",
       "   'payment')],\n",
       " [('cs shopee', 'ngasih tau', 'neutral', 'customerservice'),\n",
       "  ('cs shopee', 'bantu balikin', 'positive', 'customerservice')],\n",
       " [('toped', 'lagi gangguan', 'neutral', 'website&apps')],\n",
       " [('shopee express instant',\n",
       "   'belum terima barangnya',\n",
       "   'negative',\n",
       "   'delivery')],\n",
       " [('akun', 'error', 'negative', 'website&apps')],\n",
       " [('filter pencarian', 'tolong perbaiki bug', 'negative', 'website&apps')],\n",
       " [('shopee', 'kenapa bangat bgt anjir promonya', 'positive', 'price'),\n",
       "  ('tokped', 'kenapa bangat bgt anjir promonya', 'positive', 'price'),\n",
       "  ('lazada', 'kenapa bangat bgt anjir promonya', 'positive', 'price')],\n",
       " [('bebas ongkir', 'berpotensi menjebak', 'negative', 'delivery')],\n",
       " [('vocher ovo', 'klaim', 'neutral', 'payment'),\n",
       "  ('voucher dana', 'klaim', 'neutral', 'payment')],\n",
       " [('shopee', 'tidak bisa cod', 'neutral', 'delivery')],\n",
       " [('fitur cod', 'apus', 'negative', 'delivery')],\n",
       " [('harganya', 'turun jauh', 'positive', 'price')],\n",
       " [('email promosi', 'masih aja masuk', 'negative', 'website&apps')],\n",
       " [('admin token', 'naik lagi mahal bgt', 'negative', 'price')],\n",
       " [('gopay', 'sampai sekarang cashback belum masuk', 'negative', 'payment')],\n",
       " [('pembayaran indihome', 'gangguan', 'negative', 'payment')],\n",
       " [('komplain barang',\n",
       "   'belum diterima ke tokped',\n",
       "   'negative',\n",
       "   'customerservice')],\n",
       " [('_', 'ga respon2', 'negative', 'customerservice'),\n",
       "  ('seller', 'ga respon2', 'negative', 'product')],\n",
       " [('pesanan gagal', 'gegara sea bank', 'negative', 'payment')],\n",
       " [('live di shopee', 'candu', 'positive', 'website&apps')],\n",
       " [('bukti unboxing', 'bakal menang', 'neutral', 'product'),\n",
       "  ('shopee', 'uangnya balik', 'positive', 'product')],\n",
       " [('dana lazada', 'belum masuk', 'negative', 'payment')],\n",
       " [('ojol', 'sdm pada rendah', 'negative', 'delivery'),\n",
       "  ('ojol', 'sm maing', 'negative', 'delivery')],\n",
       " [('tokped', 'nambah ongkos kirim', 'negative', 'price')],\n",
       " [('shopee express', 'cepet', 'positive', 'delivery'),\n",
       "  ('shopee express', 'aman bgt', 'positive', 'delivery'),\n",
       "  ('anteraja', 'trauma paket ilang', 'negative', 'delivery')],\n",
       " [('situs', 'error mulu', 'negative', 'website&apps')],\n",
       " [('shopee', 'yang penting ada gratis ongkir', 'neutral', 'delivery'),\n",
       "  ('shopee', 'gapapa ga ada diskon / promo', 'neutral', 'price')],\n",
       " [('seabank', 'lagi gangguan ya?', 'neutral', 'payment')],\n",
       " [('biaya admin', 'naik uy', 'negative', 'price')],\n",
       " [('shopee xpress point', 'cari yang terdekat', 'neutral', 'payment')],\n",
       " [('tokopedia', 'gue kira ada diskon', 'neutral', 'price')],\n",
       " [('zalora', 'lama bgt pengirimannya', 'negative', 'delivery')],\n",
       " [('lazpaylater', 'tiba2 ada biaya tambahan', 'negative', 'payment')],\n",
       " [('barang hilang', 'refund belum masuk', 'negative', 'payment'),\n",
       "  ('tokped care',\n",
       "   'berkali2 followup jawabannya selalu sama',\n",
       "   'negative',\n",
       "   'customerservice')],\n",
       " [('kuliah', 'dapet cashback', 'positive', 'price')]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('pesananku',\n",
       "   'sudah bayar tapi kenapa dibatalin',\n",
       "   'negative',\n",
       "   'website&apps')],\n",
       " [('cal center', 'hanya berbelit belit', 'negative', 'customerservice'),\n",
       "  ('transaksi',\n",
       "   'tidak bisa di cancel karena sudah status menungu',\n",
       "   'negative',\n",
       "   'delivery')],\n",
       " [('alfa', 'kalau jumat-mingu ada promo', 'positive', 'price')],\n",
       " [('tokopedia care', 'sedang ganguan', 'neutral', 'website&apps')],\n",
       " [('token listrik', 'lewat tokped eror', 'negative', 'website&apps')],\n",
       " [('default paymentnya', 'tanpa ada konfirmasi ulang', 'negative', 'payment')],\n",
       " [('payday', 'bulan ini gak dapat apa-apa', 'neutral', 'product')],\n",
       " [('tokopedia', 'dapat harga paling murah', 'positive', 'price'),\n",
       "  ('shopee', 'dapat harga paling murah', 'positive', 'price')],\n",
       " [('tokopedia', 'makin ke sini makin pelit saja', 'negative', 'price')],\n",
       " [('ghd', 'lebi lancar nyatok rambutnya', 'positive', 'product'),\n",
       "  ('hairbeauron', 'lebih tersendat', 'negative', 'product')],\n",
       " [('tokopedia', 'ongkos kirim kelewat mahal', 'negative', 'delivery'),\n",
       "  ('tokopedia', 'sistem gak aman', 'negative', 'website&apps')],\n",
       " [('bebas ongkirnya', 'cimit banget sih', 'negative', 'delivery')],\n",
       " [('kurir', 'dateng2 sampai resi valid', 'negative', 'delivery')],\n",
       " [('tokped', 'selalu dapat panawaran yang murah2', 'positive', 'price'),\n",
       "  ('shope', 'selalu dapat panawaran yang murah2', 'positive', 'price')],\n",
       " [('lazada', 'nombok ongkos kirim melulu', 'negative', 'delivery')],\n",
       " [('tokopedia now',\n",
       "   'nemu toko yang lebih murah dan barangnya lengkap',\n",
       "   'negative',\n",
       "   'product')],\n",
       " [('kurir', 'paket dibawa kabur', 'negative', 'delivery')],\n",
       " [('verifikasi transfer', 'gabisa dipercepat apa', 'negative', 'payment')],\n",
       " [('tokped', 'mksh pernah kasih promo gila gilan', 'positive', 'price')],\n",
       " [('harga', 'miring', 'positif', 'price')],\n",
       " [('lazada', 'banyak tuh promo atau vouchernya', 'positive', 'price'),\n",
       "  ('lazada', 'banyak tuh promo atau vouchernya', 'positive', 'price')],\n",
       " [('tiktok shop', 'harganya lebih murah', 'neutral', 'price')],\n",
       " [('akun shope', 'gak bisa transaksi', 'negative', 'website&apps')],\n",
       " [('seller', 'lambat amat', 'negative', 'customerservice')],\n",
       " [('paylatter shope',\n",
       "   'semakin gede saja biaya admin nya',\n",
       "   'negative',\n",
       "   'payment')],\n",
       " [('shope', 'gratis ongkos kirim', 'positive', 'delivery')],\n",
       " [('belanja',\n",
       "   'masih harus repot repot mengurus ini itu ke polisi',\n",
       "   'negative',\n",
       "   'customerservice'),\n",
       "  ('belanja',\n",
       "   'tanggung jawab cuma sebatas berbagi informasi',\n",
       "   'negative',\n",
       "   'customerservice')],\n",
       " [('tokped', 'ongkirnya sekarang mehong', 'negative', 'delivery')],\n",
       " [('login tokped', 'bisa dibantu ga?', 'neutral', 'website&apps')],\n",
       " [('pengiriman', 'driver ngeluh', 'neutral', 'delivery')],\n",
       " [('ekspedisi',\n",
       "   'cara kerja kalian sekarang kurang bagus',\n",
       "   'negative',\n",
       "   'delivery')],\n",
       " [('pengiriman nextday', 'diangap terlambat', 'negative', 'delivery')],\n",
       " [('blibli',\n",
       "   'segampang itu pengirimannya area jabodetabek',\n",
       "   'positive',\n",
       "   'delivery'),\n",
       "  ('blibli',\n",
       "   'secepat itu pengirimannya area jabodetabek',\n",
       "   'positive',\n",
       "   'delivery')],\n",
       " [('voucher', 'ga paham caranya', 'neutral', 'price')],\n",
       " [('tokped', 'gercep banget udh dateng', 'positive', 'delivery'),\n",
       "  ('laadut', 'lambat banget masa datengnya hari', 'negative', 'delivery')],\n",
       " [('tokped', 'gencarnya ongkos kirim naik', 'negative', 'delivery'),\n",
       "  ('lazada', 'gencarnya ongkos kirim naik', 'negative', 'delivery'),\n",
       "  ('shoope',\n",
       "   'bahkan turun bahkan dibawah sebelum pandemi',\n",
       "   'positive',\n",
       "   'delivery')],\n",
       " [('tokped', 'sering cek saja', 'positive', 'website&apps'),\n",
       "  ('olx', 'sering cek saja', 'positive', 'website&apps')],\n",
       " [('tiket kereta', 'lagi promo jadi mengiurkan', 'positive', 'price')],\n",
       " [('the ordinary', 'sering diskon', 'positive', 'price')],\n",
       " [('livin', 'lagi ganguan kah', 'neutral', 'website&apps')],\n",
       " [('tokped', 'lama banget loading melulu', 'negative', 'website&apps')],\n",
       " [('tokopedia', 'ongkirnya gak ngotak lagi sekarang', 'negative', 'delivery')],\n",
       " [('gopaylater',\n",
       "   'apakah bisa saya bayar seelum tanggal ditiap bulanya',\n",
       "   'neutral',\n",
       "   'payment')],\n",
       " [('cs shope', 'memberikan tau', 'neutral', 'customerservice'),\n",
       "  ('cs shope', 'bantu balikin', 'positive', 'customerservice')],\n",
       " [('toped', 'lagi ganguan', 'neutral', 'website&apps')],\n",
       " [('shope expres instant', 'belum terima barangnya', 'negative', 'delivery')],\n",
       " [('akunku', 'eror', 'negative', 'website&apps')],\n",
       " [('filter pencarian produk', 'perbaiki bug', 'neutral', 'website&apps')],\n",
       " [('lazada', 'kenapa bangat bangat bangat promonya', 'positive', 'price'),\n",
       "  ('tokped', 'kenapa bangat bangat bangat promonya', 'positive', 'price'),\n",
       "  ('lazada', 'kenapa bangat bangat promonya', 'positive', 'price')],\n",
       " [('bebas ongkos kirim', 'berpotensi menjebak', 'negative', 'delivery')],\n",
       " [('vocher ovo dana lazada', 'klaim', 'neutral', 'payment'),\n",
       "  ('voucher dana lazada', 'klaim', 'neutral', 'payment')],\n",
       " [('shopee', 'tidak bisa cod', 'neutral', 'delivery')],\n",
       " [('fitur cod', 'apus', 'negative', 'delivery')],\n",
       " [('harganya', 'turun jauh', 'positive', 'price')],\n",
       " [('email promosi', 'masih saja masuk', 'negative', 'website&apps')],\n",
       " [('admin token', 'naik lagi mahal banget', 'negative', 'price')],\n",
       " [('gopay', 'sampai sekarang cashback belum masuk', 'negative', 'payment')],\n",
       " [('pembayaran indihome', 'ganguan', 'negative', 'payment')],\n",
       " [('komplain barang',\n",
       "   'belum diterima ke tokped',\n",
       "   'negative',\n",
       "   'customerservice')],\n",
       " [('_', 'ga respon2', 'negative', 'customerservice'),\n",
       "  ('seller', 'ga respon2', 'negative', 'customerservice')],\n",
       " [('pesanan gagal', 'gegara sea bank', 'negative', 'payment')],\n",
       " [('live di shope', 'candu', 'positive', 'website&apps')],\n",
       " [('bukti unboxing', 'bakal menang', 'neutral', 'product'),\n",
       "  ('shopee', 'uangnya balik', 'positive', 'product')],\n",
       " [('dana lazada', 'belum masuk', 'negative', 'payment')],\n",
       " [('ojol', 'sdm pada rendah', 'negative', 'delivery'),\n",
       "  ('ojol', 'sm maing', 'negative', 'delivery')],\n",
       " [('tokped', 'nambah ongkos kirim', 'negative', 'price')],\n",
       " [('anteraja', 'cepat aman bgt', 'positive', 'delivery'),\n",
       "  ('anteraja', 'ga ada solusi apapun', 'negative', 'delivery')],\n",
       " [('situs', 'eror melulu', 'negative', 'website&apps')],\n",
       " [('pe shope', 'yang penting ada gratis ongkos kirim', 'neutral', 'delivery'),\n",
       "  ('pe shope', 'gapapa ga ada diskon / promo', 'neutral', 'price')],\n",
       " [('seabank', 'lagi ganguan ya?', 'neutral', 'payment')],\n",
       " [('biaya admi', 'naik uy', 'negative', 'price')],\n",
       " [('xpres point', 'cari yang terdekat', 'neutral', 'payment')],\n",
       " [('tokopedia', 'eh saya kira ada diskon', 'neutral', 'price')],\n",
       " [('zalora', 'lama banget pengirimanya', 'negative', 'delivery')],\n",
       " [('lazpaylater', 'tiba2 ada biaya tambahan', 'negative', 'payment')],\n",
       " [('barang hilang', 'refund belum masuk', 'negative', 'payment'),\n",
       "  ('tokped care',\n",
       "   'berkali2 folowup jawabannya selalu sama',\n",
       "   'negative',\n",
       "   'customerservice')],\n",
       " [('kuliah', 'dapat casback', 'positive', 'price')]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
