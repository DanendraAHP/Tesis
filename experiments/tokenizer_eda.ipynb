{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danendra\\anaconda3\\envs\\tesis\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constant\n",
    "max_length = 128\n",
    "text_col = 'content'\n",
    "label_col = 'quadruplet'\n",
    "preprocess_type = 'p00'\n",
    "TOKENIZER_PATH = f'../tokenizer'\n",
    "PRETRAINED_MODEL = \"Wikidepia/IndoT5-base\"\n",
    "DATA_PATH = '../Data/quadruplet/quadruplet_annottated_sample_dataset_clean.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "df['clean_tweet'] = df['content'].apply(p.clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using old tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_count = 0\n",
    "not_splitted_tokens_count = 0\n",
    "tokens_one_word_count = 0\n",
    "tokens_two_word_count = 0\n",
    "tokens_three_word_count = 0\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    clean_tweet = row['clean_tweet']\n",
    "    tokens = old_tokenizer.tokenize(clean_tweet)\n",
    "    all_tokens_count+=len(tokens)\n",
    "    not_splitted_tokens = [tok for tok in tokens if tok.startswith('▁')]\n",
    "    not_splitted_tokens_count+=len(not_splitted_tokens)\n",
    "    splitted_tokens = [tok for tok in tokens if not tok.startswith('▁')]\n",
    "    for tok in splitted_tokens:\n",
    "        if len(tok)==1:\n",
    "            tokens_one_word_count+=1\n",
    "        elif len(tok)==2:\n",
    "            tokens_two_word_count+=1\n",
    "        else:\n",
    "            tokens_three_word_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36779, 20326, 5692, 4459, 6302)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens_count, not_splitted_tokens_count, tokens_one_word_count, tokens_two_word_count, tokens_three_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_splitted_tokens_count_perc = round(not_splitted_tokens_count/all_tokens_count*100,3)\n",
    "tokens_one_word_count_perc = round(tokens_one_word_count/all_tokens_count*100, 3)\n",
    "tokens_two_word_count_perc = round(tokens_two_word_count/all_tokens_count*100, 3)\n",
    "tokens_three_word_count_perc = round(tokens_three_word_count/all_tokens_count*100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tokens from tokenizer 36779\n",
      "all tokens not splitted from tokenizer 20326 or 55.265 percent from total tokens\n",
      "all tokens from tokenizer that splitted into only one character 5692 or 15.476 percent from total tokens\n",
      "all tokens from tokenizer that splitted into two characters 4459 or 12.124 percent from total tokens\n",
      "all tokens from tokenizer that splitted into three characters or more 6302 or 17.135 percent from total tokens\n"
     ]
    }
   ],
   "source": [
    "print(f'all tokens from tokenizer {all_tokens_count}')\n",
    "print(f'all tokens not splitted from tokenizer {not_splitted_tokens_count} or {not_splitted_tokens_count_perc} percent from total tokens')\n",
    "print(f'all tokens from tokenizer that splitted into only one character {tokens_one_word_count} or {tokens_one_word_count_perc} percent from total tokens')\n",
    "print(f'all tokens from tokenizer that splitted into two characters {tokens_two_word_count} or {tokens_two_word_count_perc} percent from total tokens')\n",
    "print(f'all tokens from tokenizer that splitted into three characters or more {tokens_three_word_count} or {tokens_three_word_count_perc} percent from total tokens')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using new tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_count = 0\n",
    "not_splitted_tokens_count = 0\n",
    "tokens_one_word_count = 0\n",
    "tokens_two_word_count = 0\n",
    "tokens_three_word_count = 0\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    clean_tweet = row['clean_tweet']\n",
    "    tokens = new_tokenizer.tokenize(clean_tweet)\n",
    "    all_tokens_count+=len(tokens)\n",
    "    not_splitted_tokens = [tok for tok in tokens if tok.startswith('▁')]\n",
    "    not_splitted_tokens_count+=len(not_splitted_tokens)\n",
    "    splitted_tokens = [tok for tok in tokens if not tok.startswith('▁')]\n",
    "    for tok in splitted_tokens:\n",
    "        if len(tok)==1:\n",
    "            tokens_one_word_count+=1\n",
    "        elif len(tok)==2:\n",
    "            tokens_two_word_count+=1\n",
    "        else:\n",
    "            tokens_three_word_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23846, 20326, 1572, 500, 1448)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens_count, not_splitted_tokens_count, tokens_one_word_count, tokens_two_word_count, tokens_three_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_splitted_tokens_count_perc = round(not_splitted_tokens_count/all_tokens_count*100,3)\n",
    "tokens_one_word_count_perc = round(tokens_one_word_count/all_tokens_count*100, 3)\n",
    "tokens_two_word_count_perc = round(tokens_two_word_count/all_tokens_count*100, 3)\n",
    "tokens_three_word_count_perc = round(tokens_three_word_count/all_tokens_count*100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tokens from tokenizer 23846\n",
      "all tokens not splitted from tokenizer 20326 or 85.239 percent from total tokens\n",
      "all tokens from tokenizer that splitted into only one character 1572 or 6.592 percent from total tokens\n",
      "all tokens from tokenizer that splitted into two characters 500 or 2.097 percent from total tokens\n",
      "all tokens from tokenizer that splitted into three characters or more 1448 or 6.072 percent from total tokens\n"
     ]
    }
   ],
   "source": [
    "print(f'all tokens from tokenizer {all_tokens_count}')\n",
    "print(f'all tokens not splitted from tokenizer {not_splitted_tokens_count} or {not_splitted_tokens_count_perc} percent from total tokens')\n",
    "print(f'all tokens from tokenizer that splitted into only one character {tokens_one_word_count} or {tokens_one_word_count_perc} percent from total tokens')\n",
    "print(f'all tokens from tokenizer that splitted into two characters {tokens_two_word_count} or {tokens_two_word_count_perc} percent from total tokens')\n",
    "print(f'all tokens from tokenizer that splitted into three characters or more {tokens_three_word_count} or {tokens_three_word_count_perc} percent from total tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1535, 2394, 7, 1]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('saya sukaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize('saya sukaaaaa makan anjingmakan')\n",
    "splitted_word = [tok for tok in tokens if not tok.startswith('▁')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'a', 'a', 'ma', 'kan']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
