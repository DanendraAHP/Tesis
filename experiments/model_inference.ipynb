{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import preprocessor as p\n",
    "from slang_word import SLANG_WORDS\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_repetitive(word):\n",
    "    prev_char = None\n",
    "    char_count=-1\n",
    "    clean_word=''\n",
    "    for c in word:\n",
    "        if prev_char!=c:\n",
    "            prev_char=c\n",
    "            char_count=0\n",
    "        else:\n",
    "            char_count+=1\n",
    "        if char_count<1:\n",
    "            clean_word+=c\n",
    "    #remove word if only 1 char left\n",
    "    return clean_word if len(clean_word)>1 else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #lower case\n",
    "    text = text.lower()\n",
    "    #clean text with tweet-preprocessor\n",
    "    text = p.clean(text)\n",
    "    #clean repetitive word\n",
    "    text = \" \".join([clean_repetitive(word) for word in text.split()])\n",
    "    #convert slang word into dictionary\n",
    "    text = \" \".join([SLANG_WORDS[word] if word in SLANG_WORDS else word for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "def stem(text):\n",
    "    return stemmer.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create stopword remover\n",
    "stop_factory = StopWordRemoverFactory()\n",
    "stopword_remover = stop_factory.create_stop_word_remover()\n",
    "def stopword_removal(text):\n",
    "    return stopword_remover.remove(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(examples):\n",
    "    inputs = examples[TEXT_COL]\n",
    "    inputs = [clean_text(input) for input in inputs]\n",
    "    if PREPROCESS=='p02' or PREPROCESS=='p04':\n",
    "        inputs = [stopword_removal(input) for input in inputs]\n",
    "    if PREPROCESS=='p03' or PREPROCESS=='p04':\n",
    "        inputs = [stem(input) for input in inputs]\n",
    "    targets =examples[\"quadruplet\"] \n",
    "    tokenized_inputs = tokenizer(\n",
    "        inputs, text_target=targets, max_length=max_length, truncation=True\n",
    "    )\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Wikidepia/IndoT5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained('models/tf-indot5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "raw_dataset = load_dataset('csv', data_files='../Data/quadruplet_only.csv', split='train')\n",
    "tokenized_dataset = raw_dataset.map(preprocess_dataset, batched=True, remove_columns=raw_dataset.column_names)\n",
    "splitted_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "generated_text = model.generate(splitted_dataset['test']['input_ids'], max_length=max_length)\n",
    "pred_text = tokenizer.batch_decode(generated_text, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = splitted_dataset['test'].to_pandas()\n",
    "test_df['pred_quadruplet_pt_bart'] = pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('data/quadruplet_test_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
