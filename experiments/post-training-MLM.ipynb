{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danendra\\anaconda3\\envs\\tesis\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    BartForConditionalGeneration,\n",
    ")\n",
    "import collections\n",
    "import numpy as np\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "from indobenchmark import IndoNLGTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kalau t5\n",
    "MODEL_CHECKPOINT = \"../models/pt-indot5-TA_PT\"\n",
    "TOK_CHECKPOINT = \"Wikidepia/IndoT5-base\"\n",
    "SAVE_PATH = \"../models/pt-indot5-MLM_TA_PT\"\n",
    "#kalau bart\n",
    "# MODEL_CHECKPOINT = \"indobenchmark/indobart-v2\"\n",
    "# SAVE_PATH = \"models/pt-indobart-MLM_PT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the t5 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOK_CHECKPOINT)\n",
    "\n",
    "#set up bart tokenizer\n",
    "#tokenizer = IndoNLGTokenizer.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"clean_tweet\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // CHUNK_SIZE) * CHUNK_SIZE\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + CHUNK_SIZE] for i in range(0, total_length, CHUNK_SIZE)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate_id(tokens, highest_id):\n",
    "    extra_ids = tokenizer.convert_tokens_to_ids([f'<extra_id_{id}>' for id in range(highest_id)])\n",
    "    extra_ids = {key:0 for key in extra_ids}\n",
    "    del_idx = []\n",
    "    for i,tok in enumerate(tokens):\n",
    "        if tok in extra_ids:\n",
    "            extra_ids[tok]+=1\n",
    "            if extra_ids[tok]>1:\n",
    "                del_idx.append(i)\n",
    "    new_tokens = [tokens[i] for i in range(len(tokens)) if i not in del_idx]\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def multi_word_masking(examples):\n",
    "    new_inputs = []\n",
    "    new_labels = []\n",
    "    for i in range(len(examples['input_ids'])):\n",
    "        word_ids = examples['word_ids'][i]\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = examples[\"input_ids\"][i]\n",
    "        labels = examples[\"labels\"][i]\n",
    "        input_masked = -1\n",
    "        label_masked = -1\n",
    "        prev_mask = None\n",
    "        for word_id, masked in enumerate(mask):\n",
    "            if prev_mask!=masked:\n",
    "                if masked==0:\n",
    "                    label_masked+=1\n",
    "                elif masked==1:\n",
    "                    input_masked+=1\n",
    "                prev_mask=masked\n",
    "            for idx in mapping[word_id]:\n",
    "                #if not masking input_ids then we mask the label\n",
    "                if masked==0:\n",
    "                    labels[idx] = tokenizer.convert_tokens_to_ids(f'<extra_id_{label_masked}>')\n",
    "                #if masking then we mask the input_ids\n",
    "                elif masked==1:\n",
    "                    input_ids[idx] = tokenizer.convert_tokens_to_ids(f'<extra_id_{input_masked}>')\n",
    "        #dropping the same extra_id\n",
    "        input_ids = drop_duplicate_id(input_ids, input_masked+1)\n",
    "        labels = drop_duplicate_id(labels, label_masked+1)\n",
    "        new_inputs.append(input_ids)\n",
    "        new_labels.append(labels)\n",
    "    examples['labels'] = new_labels\n",
    "    examples['input_ids'] = new_inputs\n",
    "    examples['attention_mask'] = [[1 for i in range(len(examples['input_ids'][j]))] for j in range(len(examples['input_ids']))]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/danendra/.cache/huggingface/datasets/csv/default-a0da231f93618037/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 1/1 [00:00<00:00, 34.50it/s]\n",
      "Loading cached processed dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-a0da231f93618037\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-ecfd2fdf1dcb97c8.arrow\n",
      "Loading cached processed dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-a0da231f93618037\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-c68c1a0da8738c95.arrow\n",
      "Loading cached split indices for dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-a0da231f93618037\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-8e9f8a7eceb1a98c.arrow and C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-a0da231f93618037\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-756e5054145a7f4d.arrow\n",
      "Loading cached processed dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-a0da231f93618037\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-2e3ec2b53473b039.arrow\n",
      "Loading cached processed dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-a0da231f93618037\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-014071b99d342f5b.arrow\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset('csv', data_files='../Data/post-train/MLM/clean_tweet.csv')\n",
    "tokenized_datasets = raw_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=raw_dataset['train'].column_names\n",
    ")\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "downsampled_dataset = downsampled_dataset.map(multi_word_masking, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naksir bgt kulot<extra_id_0> Shopee tp mau beli mikir kali karena<extra_id_1> ngestalk lagi.<extra_id_2> gue beneran naksir ama kulotnya</s><extra_id_3> ga lagi deh pake<extra_id_4> express, kapok :\")</s><extra_id_5> emang set<extra_id_6> xpress aja.. tapi setidaknya aku pake yang standard huuuu orang<extra_id_7> free bodoh<extra_id_8> ga cek</s> <unk> gapake bukalapak kakk coba wa<extra_id_9></s> <unk>Langit Merah Jakarta,'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(downsampled_dataset['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_0> di<extra_id_1> takut dianggap<extra_id_2> padahal<extra_id_3></s> <unk>udahlah<extra_id_4> shopee<extra_id_5></s> <unk>tapi sellernya<extra_id_6> shopee<extra_id_7> sama\"<extra_id_8> banget<extra_id_9></s> aja</s><extra_id_10>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(downsampled_dataset['train'][0]['labels'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MLM task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_checkpoint = \"Wikidepia/IndoT5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_CHECKPOINT).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacollator = DataCollatorForSeq2Seq(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "#model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    SAVE_PATH,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    resume_from_checkpoint=True,\n",
    "    num_train_epochs=10,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=datacollator,\n",
    "    \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perplexity before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245/245 [00:15<00:00, 15.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 92.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danendra\\anaconda3\\envs\\tesis\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  2%|▏         | 500/22020 [02:12<1:35:09,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2899, 'learning_rate': 1.954586739327884e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1000/22020 [04:24<1:33:08,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7823, 'learning_rate': 1.9091734786557677e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1500/22020 [06:34<1:27:14,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.592, 'learning_rate': 1.8637602179836514e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2000/22020 [08:35<1:17:40,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4979, 'learning_rate': 1.818346957311535e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 10%|█         | 2202/22020 [09:37<1:05:41,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2875609397888184, 'eval_runtime': 14.2432, 'eval_samples_per_second': 137.399, 'eval_steps_per_second': 17.201, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 2500/22020 [11:09<1:14:36,  4.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4182, 'learning_rate': 1.772933696639419e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 3000/22020 [13:04<1:13:22,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3417, 'learning_rate': 1.7275204359673027e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3500/22020 [14:59<1:11:23,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3218, 'learning_rate': 1.6821071752951864e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4000/22020 [16:55<1:09:17,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2735, 'learning_rate': 1.63669391462307e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 20%|██        | 4404/22020 [18:42<1:01:20,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.086479663848877, 'eval_runtime': 13.8511, 'eval_samples_per_second': 141.288, 'eval_steps_per_second': 17.688, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4500/22020 [19:30<1:05:16,  4.47it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2296, 'learning_rate': 1.591280653950954e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5000/22020 [21:34<1:08:38,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1842, 'learning_rate': 1.5458673932788377e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 5500/22020 [23:33<1:04:40,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1768, 'learning_rate': 1.5004541326067212e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 6000/22020 [25:32<1:03:01,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1394, 'learning_rate': 1.455040871934605e-05, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 6500/22020 [27:31<1:05:24,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1252, 'learning_rate': 1.4096276112624887e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 30%|███       | 6606/22020 [28:11<52:35,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.009526252746582, 'eval_runtime': 14.0508, 'eval_samples_per_second': 139.28, 'eval_steps_per_second': 17.437, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7000/22020 [30:13<1:06:52,  3.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1046, 'learning_rate': 1.3642143505903725e-05, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 7500/22020 [32:15<1:03:50,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0819, 'learning_rate': 1.3188010899182562e-05, 'epoch': 3.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 8000/22020 [34:14<54:19,  4.30it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0623, 'learning_rate': 1.27338782924614e-05, 'epoch': 3.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 8500/22020 [36:12<51:45,  4.35it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0691, 'learning_rate': 1.2279745685740236e-05, 'epoch': 3.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 40%|████      | 8808/22020 [37:39<45:39,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9624103307724, 'eval_runtime': 14.1465, 'eval_samples_per_second': 138.338, 'eval_steps_per_second': 17.319, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 9000/22020 [38:46<50:09,  4.33it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0562, 'learning_rate': 1.1825613079019073e-05, 'epoch': 4.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 9500/22020 [40:43<49:01,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0304, 'learning_rate': 1.137148047229791e-05, 'epoch': 4.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 10000/22020 [42:40<47:20,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0212, 'learning_rate': 1.0917347865576748e-05, 'epoch': 4.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 10500/22020 [44:43<51:04,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0161, 'learning_rate': 1.0463215258855586e-05, 'epoch': 4.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 11000/22020 [46:54<47:25,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9884, 'learning_rate': 1.0009082652134423e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 50%|█████     | 11010/22020 [47:13<43:56,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9358924627304077, 'eval_runtime': 15.6354, 'eval_samples_per_second': 125.164, 'eval_steps_per_second': 15.67, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 11500/22020 [49:43<47:33,  3.69it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9954, 'learning_rate': 9.554950045413262e-06, 'epoch': 5.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 12000/22020 [51:46<38:44,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9707, 'learning_rate': 9.1008174386921e-06, 'epoch': 5.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 12500/22020 [53:45<37:02,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9604, 'learning_rate': 8.646684831970936e-06, 'epoch': 5.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 13000/22020 [55:44<34:57,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9853, 'learning_rate': 8.192552225249773e-06, 'epoch': 5.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 60%|██████    | 13212/22020 [56:49<30:46,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9152294397354126, 'eval_runtime': 14.3072, 'eval_samples_per_second': 136.785, 'eval_steps_per_second': 17.124, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 13500/22020 [58:21<33:34,  4.23it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9704, 'learning_rate': 7.73841961852861e-06, 'epoch': 6.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 14000/22020 [1:00:20<32:16,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9518, 'learning_rate': 7.284287011807448e-06, 'epoch': 6.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 14500/22020 [1:02:19<29:27,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9625, 'learning_rate': 6.8301544050862855e-06, 'epoch': 6.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 15000/22020 [1:04:19<27:49,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9395, 'learning_rate': 6.376021798365123e-06, 'epoch': 6.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 70%|███████   | 15414/22020 [1:06:13<22:54,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9046376943588257, 'eval_runtime': 14.3251, 'eval_samples_per_second': 136.614, 'eval_steps_per_second': 17.103, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 15500/22020 [1:06:59<25:41,  4.23it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9325, 'learning_rate': 5.9218891916439605e-06, 'epoch': 7.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 16000/22020 [1:08:58<23:52,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9173, 'learning_rate': 5.467756584922798e-06, 'epoch': 7.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 16500/22020 [1:10:57<21:27,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9304, 'learning_rate': 5.013623978201635e-06, 'epoch': 7.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 17000/22020 [1:12:56<19:53,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9256, 'learning_rate': 4.559491371480473e-06, 'epoch': 7.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 17500/22020 [1:14:54<17:38,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9416, 'learning_rate': 4.1053587647593104e-06, 'epoch': 7.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 80%|████████  | 17616/22020 [1:15:36<15:32,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8973430395126343, 'eval_runtime': 14.3092, 'eval_samples_per_second': 136.765, 'eval_steps_per_second': 17.122, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 18000/22020 [1:17:33<15:46,  4.25it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9251, 'learning_rate': 3.6512261580381475e-06, 'epoch': 8.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 18500/22020 [1:19:32<13:53,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8973, 'learning_rate': 3.197093551316985e-06, 'epoch': 8.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 19000/22020 [1:21:31<12:06,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9142, 'learning_rate': 2.742960944595822e-06, 'epoch': 8.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 19500/22020 [1:23:29<09:58,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9092, 'learning_rate': 2.2888283378746596e-06, 'epoch': 8.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 90%|█████████ | 19818/22020 [1:25:04<08:33,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8934144973754883, 'eval_runtime': 15.9221, 'eval_samples_per_second': 122.911, 'eval_steps_per_second': 15.387, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 20000/22020 [1:26:17<08:55,  3.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9186, 'learning_rate': 1.8346957311534968e-06, 'epoch': 9.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 20500/22020 [1:28:30<06:47,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9026, 'learning_rate': 1.3805631244323345e-06, 'epoch': 9.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 21000/22020 [1:30:42<04:01,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8904, 'learning_rate': 9.264305177111717e-07, 'epoch': 9.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 21500/22020 [1:32:39<02:02,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9046, 'learning_rate': 4.722979109900091e-07, 'epoch': 9.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 22000/22020 [1:34:35<00:04,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9098, 'learning_rate': 1.8165304268846506e-08, 'epoch': 9.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "100%|██████████| 22020/22020 [1:34:54<00:00,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8925002813339233, 'eval_runtime': 14.1359, 'eval_samples_per_second': 138.442, 'eval_steps_per_second': 17.332, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22020/22020 [1:35:14<00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5714.9929, 'train_samples_per_second': 30.812, 'train_steps_per_second': 3.853, 'train_loss': 2.0988503976695436, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22020, training_loss=2.0988503976695436, metrics={'train_runtime': 5714.9929, 'train_samples_per_second': 30.812, 'train_steps_per_second': 3.853, 'train_loss': 2.0988503976695436, 'epoch': 10.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perplexity after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245/245 [00:14<00:00, 17.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 6.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245/245 [00:14<00:00, 17.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 6.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# eval_results = trainer.evaluate()\n",
    "# print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/danendra/.cache/huggingface/datasets/csv/default-5ee63b2ab57cbf46/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "                                                   \r"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset('csv', data_files='data/quadruplet_only.csv')\n",
    "tokenized_datasets = raw_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=raw_dataset['train'].column_names\n",
    ")\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 72\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' beli barang di dengan pengiriman pake si driver ngeluh hanya dapat perak sya sich gak percaya, masa sich sya jawab gitu, lah wong sy bayar rb sameday dari toko didepok, gk tau dah. intinya ada dua driver gosend, dan setelah gue liat kejadiannya di cctv kantor.. asumsi gue itu tu komplotan driver. pas baca thread ini, ya kok yakinnya malah penjualnya yang main. wkwkwk damn, iya bro. kitanya sebagai buyer jadi rugi. dia enak, tinggal ajuin klaim hilang terus dapet dana kita. sedangkan kita, harus nunggu approval dulu dan proses klaim segala macem. sial emang. gk mau buruk sangka'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(downsampled_dataset['train'][1]['input_ids'], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' beli barang di dengan pengiriman pake si driver ngeluh hanya dapat perak sya sich gak percaya, masa sich sya jawab gitu, lah wong sy bayar rb sameday dari toko didepok, gk tau dah',\n",
       " ' intinya ada dua driver gosend, dan setelah gue liat kejadiannya di cctv kantor',\n",
       " '',\n",
       " ' asumsi gue itu tu komplotan driver',\n",
       " ' pas baca thread ini, ya kok yakinnya malah penjualnya yang main',\n",
       " ' wkwkwk damn, iya bro',\n",
       " ' kitanya sebagai buyer jadi rugi',\n",
       " ' dia enak, tinggal ajuin klaim hilang terus dapet dana kita',\n",
       " ' sedangkan kita, harus nunggu approval dulu dan proses klaim segala macem',\n",
       " ' sial emang',\n",
       " ' gk mau buruk sangka']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(downsampled_dataset['train'][1]['input_ids'], skip_special_tokens=False).split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def text_infilling(examples):\n",
    "    new_inputs = []\n",
    "    new_labels = []\n",
    "    for i in range(len(examples['input_ids'])):\n",
    "        word_ids = examples['word_ids'][i]\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = examples[\"input_ids\"][i]\n",
    "        labels = examples[\"labels\"][i]\n",
    "        new_inputs = []\n",
    "        new_labels = []\n",
    "        for word_id, masked in enumerate(mask):\n",
    "            for idx in mapping[word_id]:\n",
    "                masked_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        #dropping the same extra_id\n",
    "        input_ids = drop_duplicate_id(input_ids, input_masked+1)\n",
    "        labels = drop_duplicate_id(labels, label_masked+1)\n",
    "        new_inputs.append(input_ids)\n",
    "        new_labels.append(labels)\n",
    "    examples['labels'] = new_labels\n",
    "    examples['input_ids'] = new_inputs\n",
    "    examples['attention_mask'] = [[1 for i in range(len(examples['input_ids'][j]))] for j in range(len(examples['input_ids']))]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wwm_probability = 0.2\n",
    "\n",
    "\n",
    "# def whole_word_masking(examples):\n",
    "#     new_inputs = []\n",
    "#     new_labels = []\n",
    "#     for i in range(len(examples['input_ids'])):\n",
    "#         word_ids = examples['word_ids'][i]\n",
    "#         # Create a map between words and corresponding token indices\n",
    "#         mapping = collections.defaultdict(list)\n",
    "#         current_word_index = -1\n",
    "#         current_word = None\n",
    "#         for idx, word_id in enumerate(word_ids):\n",
    "#             if word_id is not None:\n",
    "#                 if word_id != current_word:\n",
    "#                     current_word = word_id\n",
    "#                     current_word_index += 1\n",
    "#                 mapping[current_word_index].append(idx)\n",
    "\n",
    "#         # Randomly mask words\n",
    "#         mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "#         input_ids = examples[\"input_ids\"][i]\n",
    "#         labels = examples[\"labels\"][i]\n",
    "#         input_masked = 0\n",
    "#         label_masked = 0\n",
    "#         for word_id, masked in enumerate(mask):\n",
    "#             for idx in mapping[word_id]:\n",
    "#                 #if not masking input_ids then we mask the label\n",
    "#                 if masked==0:\n",
    "#                     labels[idx] = tokenizer.convert_tokens_to_ids(f'<extra_id_{label_masked}>')\n",
    "#                 #if masking then we mask the input_ids\n",
    "#                 elif masked==1:\n",
    "#                     input_ids[idx] = tokenizer.convert_tokens_to_ids(f'<extra_id_{input_masked}>')\n",
    "#             if masked==0:\n",
    "#                 label_masked+=1\n",
    "#             elif masked==1:\n",
    "#                 input_masked+=1\n",
    "#         new_inputs.append(input_ids)\n",
    "#         new_labels.append(labels)\n",
    "#     examples['labels'] = new_labels\n",
    "#     examples['input_ids'] = new_inputs\n",
    "#     return examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
