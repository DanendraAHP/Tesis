{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danendra\\anaconda3\\envs\\tesis\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    BartForConditionalGeneration,\n",
    ")\n",
    "import collections\n",
    "import numpy as np\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "from indobenchmark import IndoNLGTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kalau t5\n",
    "MODEL_CHECKPOINT = \"Wikidepia/IndoT5-base\"\n",
    "SAVE_PATH = \"../models/pt-indot5-MLM_PT\"\n",
    "#kalau bart\n",
    "# MODEL_CHECKPOINT = \"indobenchmark/indobart-v2\"\n",
    "# SAVE_PATH = \"models/pt-indobart-MLM_PT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the t5 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "#set up bart tokenizer\n",
    "#tokenizer = IndoNLGTokenizer.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"clean_tweet\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // CHUNK_SIZE) * CHUNK_SIZE\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + CHUNK_SIZE] for i in range(0, total_length, CHUNK_SIZE)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate_id(tokens, highest_id):\n",
    "    extra_ids = tokenizer.convert_tokens_to_ids([f'<extra_id_{id}>' for id in range(highest_id)])\n",
    "    extra_ids = {key:0 for key in extra_ids}\n",
    "    del_idx = []\n",
    "    for i,tok in enumerate(tokens):\n",
    "        if tok in extra_ids:\n",
    "            extra_ids[tok]+=1\n",
    "            if extra_ids[tok]>1:\n",
    "                del_idx.append(i)\n",
    "    new_tokens = [tokens[i] for i in range(len(tokens)) if i not in del_idx]\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def multi_word_masking(examples):\n",
    "    new_inputs = []\n",
    "    new_labels = []\n",
    "    for i in range(len(examples['input_ids'])):\n",
    "        word_ids = examples['word_ids'][i]\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = examples[\"input_ids\"][i]\n",
    "        labels = examples[\"labels\"][i]\n",
    "        input_masked = -1\n",
    "        label_masked = -1\n",
    "        prev_mask = None\n",
    "        for word_id, masked in enumerate(mask):\n",
    "            if prev_mask!=masked:\n",
    "                if masked==0:\n",
    "                    label_masked+=1\n",
    "                elif masked==1:\n",
    "                    input_masked+=1\n",
    "                prev_mask=masked\n",
    "            for idx in mapping[word_id]:\n",
    "                #if not masking input_ids then we mask the label\n",
    "                if masked==0:\n",
    "                    labels[idx] = tokenizer.convert_tokens_to_ids(f'<extra_id_{label_masked}>')\n",
    "                #if masking then we mask the input_ids\n",
    "                elif masked==1:\n",
    "                    input_ids[idx] = tokenizer.convert_tokens_to_ids(f'<extra_id_{input_masked}>')\n",
    "        #dropping the same extra_id\n",
    "        input_ids = drop_duplicate_id(input_ids, input_masked+1)\n",
    "        labels = drop_duplicate_id(labels, label_masked+1)\n",
    "        new_inputs.append(input_ids)\n",
    "        new_labels.append(labels)\n",
    "    examples['labels'] = new_labels\n",
    "    examples['input_ids'] = new_inputs\n",
    "    examples['attention_mask'] = [[1 for i in range(len(examples['input_ids'][j]))] for j in range(len(examples['input_ids']))]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/danendra/.cache/huggingface/datasets/csv/default-a0da231f93618037/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 1/1 [00:00<00:00, 333.38it/s]\n",
      "Loading cached processed dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-a0da231f93618037\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-ecfd2fdf1dcb97c8.arrow\n",
      "Loading cached processed dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-a0da231f93618037\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-c68c1a0da8738c95.arrow\n",
      "Loading cached split indices for dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-a0da231f93618037\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-8e9f8a7eceb1a98c.arrow and C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-a0da231f93618037\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-756e5054145a7f4d.arrow\n",
      "Loading cached processed dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-a0da231f93618037\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-2e3ec2b53473b039.arrow\n",
      "Loading cached processed dataset at C:\\Users\\danendra\\.cache\\huggingface\\datasets\\csv\\default-a0da231f93618037\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-014071b99d342f5b.arrow\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset('csv', data_files='../Data/post-train/MLM/clean_tweet.csv')\n",
    "tokenized_datasets = raw_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=raw_dataset['train'].column_names\n",
    ")\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "downsampled_dataset = downsampled_dataset.map(multi_word_masking, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naksir bgt kulot<extra_id_0> Shopee tp mau beli mikir kali karena<extra_id_1> ngestalk lagi.<extra_id_2> gue beneran naksir ama kulotnya</s><extra_id_3> ga lagi deh pake<extra_id_4> express, kapok :\")</s><extra_id_5> emang set<extra_id_6> xpress aja.. tapi setidaknya aku pake yang standard huuuu orang<extra_id_7> free bodoh<extra_id_8> ga cek</s> <unk> gapake bukalapak kakk coba wa<extra_id_9></s> <unk>Langit Merah Jakarta,'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(downsampled_dataset['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_0> di<extra_id_1> takut dianggap<extra_id_2> padahal<extra_id_3></s> <unk>udahlah<extra_id_4> shopee<extra_id_5></s> <unk>tapi sellernya<extra_id_6> shopee<extra_id_7> sama\"<extra_id_8> banget<extra_id_9></s> aja</s><extra_id_10>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(downsampled_dataset['train'][0]['labels'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MLM task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Wikidepia/IndoT5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacollator = DataCollatorForSeq2Seq(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "#model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    SAVE_PATH,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    resume_from_checkpoint=True,\n",
    "    num_train_epochs=10,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=datacollator,\n",
    "    \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perplexity before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 245/245 [00:14<00:00, 17.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 68.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danendra\\anaconda3\\envs\\tesis\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  2%|▏         | 500/22020 [02:00<1:26:12,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.185, 'learning_rate': 1.954586739327884e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1000/22020 [04:00<1:26:31,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7884, 'learning_rate': 1.9091734786557677e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1500/22020 [06:01<1:21:51,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.616, 'learning_rate': 1.8637602179836514e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2000/22020 [08:01<1:20:59,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5219, 'learning_rate': 1.818346957311535e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 10%|█         | 2202/22020 [09:04<1:11:49,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.31101655960083, 'eval_runtime': 14.4467, 'eval_samples_per_second': 135.463, 'eval_steps_per_second': 16.959, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 2500/22020 [10:38<1:17:42,  4.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4364, 'learning_rate': 1.772933696639419e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 3000/22020 [12:40<1:20:22,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3576, 'learning_rate': 1.7275204359673027e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3500/22020 [14:48<1:16:36,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3393, 'learning_rate': 1.6821071752951864e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4000/22020 [16:52<1:11:14,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2893, 'learning_rate': 1.63669391462307e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 20%|██        | 4404/22020 [18:48<1:03:25,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.100149393081665, 'eval_runtime': 14.6455, 'eval_samples_per_second': 133.625, 'eval_steps_per_second': 16.729, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4500/22020 [19:33<1:10:25,  4.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2445, 'learning_rate': 1.591280653950954e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5000/22020 [21:35<1:10:38,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1982, 'learning_rate': 1.5458673932788377e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 5500/22020 [23:40<1:07:07,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1873, 'learning_rate': 1.5004541326067212e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 6000/22020 [25:47<1:12:05,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1502, 'learning_rate': 1.455040871934605e-05, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 6500/22020 [27:52<1:03:07,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1344, 'learning_rate': 1.4096276112624887e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 30%|███       | 6606/22020 [28:33<56:00,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.018792152404785, 'eval_runtime': 14.663, 'eval_samples_per_second': 133.465, 'eval_steps_per_second': 16.709, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7000/22020 [30:34<1:00:33,  4.13it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1127, 'learning_rate': 1.3642143505903725e-05, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 7500/22020 [32:35<58:07,  4.16it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.091, 'learning_rate': 1.3188010899182562e-05, 'epoch': 3.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 8000/22020 [34:38<55:28,  4.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0696, 'learning_rate': 1.27338782924614e-05, 'epoch': 3.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 8500/22020 [36:39<53:29,  4.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0741, 'learning_rate': 1.2279745685740236e-05, 'epoch': 3.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 40%|████      | 8808/22020 [38:07<47:19,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9710631370544434, 'eval_runtime': 14.4298, 'eval_samples_per_second': 135.622, 'eval_steps_per_second': 16.979, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 9000/22020 [39:18<51:27,  4.22it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0616, 'learning_rate': 1.1825613079019073e-05, 'epoch': 4.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 9500/22020 [41:18<49:47,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0357, 'learning_rate': 1.137148047229791e-05, 'epoch': 4.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 10000/22020 [43:18<48:25,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0267, 'learning_rate': 1.0917347865576748e-05, 'epoch': 4.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 10500/22020 [45:18<46:52,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0212, 'learning_rate': 1.0463215258855586e-05, 'epoch': 4.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 11000/22020 [47:19<43:56,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9918, 'learning_rate': 1.0009082652134423e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 50%|█████     | 11010/22020 [47:35<39:43,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.94117271900177, 'eval_runtime': 14.5555, 'eval_samples_per_second': 134.451, 'eval_steps_per_second': 16.832, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 11500/22020 [49:56<42:01,  4.17it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0009, 'learning_rate': 9.554950045413262e-06, 'epoch': 5.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 12000/22020 [51:56<39:37,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9751, 'learning_rate': 9.1008174386921e-06, 'epoch': 5.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 12500/22020 [53:57<37:49,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9649, 'learning_rate': 8.646684831970936e-06, 'epoch': 5.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 13000/22020 [55:57<35:33,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9901, 'learning_rate': 8.192552225249773e-06, 'epoch': 5.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 60%|██████    | 13212/22020 [57:04<42:17,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9209895133972168, 'eval_runtime': 15.0948, 'eval_samples_per_second': 129.647, 'eval_steps_per_second': 16.231, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 13500/22020 [58:38<36:24,  3.90it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9754, 'learning_rate': 7.73841961852861e-06, 'epoch': 6.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 14000/22020 [1:00:41<32:43,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9575, 'learning_rate': 7.284287011807448e-06, 'epoch': 6.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 14500/22020 [1:02:43<29:57,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9666, 'learning_rate': 6.8301544050862855e-06, 'epoch': 6.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 15000/22020 [1:04:48<28:24,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9446, 'learning_rate': 6.376021798365123e-06, 'epoch': 6.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 70%|███████   | 15414/22020 [1:06:46<24:26,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9091694355010986, 'eval_runtime': 14.7947, 'eval_samples_per_second': 132.278, 'eval_steps_per_second': 16.56, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 15500/22020 [1:07:28<27:24,  3.97it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9354, 'learning_rate': 5.9218891916439605e-06, 'epoch': 7.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 16000/22020 [1:09:33<25:52,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.922, 'learning_rate': 5.467756584922798e-06, 'epoch': 7.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 16500/22020 [1:11:38<22:25,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9335, 'learning_rate': 5.013623978201635e-06, 'epoch': 7.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 17000/22020 [1:13:41<20:22,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9293, 'learning_rate': 4.559491371480473e-06, 'epoch': 7.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 17500/22020 [1:15:46<18:48,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9464, 'learning_rate': 4.1053587647593104e-06, 'epoch': 7.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 80%|████████  | 17616/22020 [1:16:30<16:11,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9012364149093628, 'eval_runtime': 14.9483, 'eval_samples_per_second': 130.918, 'eval_steps_per_second': 16.39, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 18000/22020 [1:18:28<16:22,  4.09it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9282, 'learning_rate': 3.6512261580381475e-06, 'epoch': 8.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 18500/22020 [1:20:33<14:33,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9012, 'learning_rate': 3.197093551316985e-06, 'epoch': 8.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 19000/22020 [1:22:39<12:40,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9173, 'learning_rate': 2.742960944595822e-06, 'epoch': 8.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 19500/22020 [1:24:47<10:42,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.913, 'learning_rate': 2.2888283378746596e-06, 'epoch': 8.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 90%|█████████ | 19818/22020 [1:26:22<08:08,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.896700143814087, 'eval_runtime': 15.0437, 'eval_samples_per_second': 130.088, 'eval_steps_per_second': 16.286, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 20000/22020 [1:27:29<08:21,  4.03it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9224, 'learning_rate': 1.8346957311534968e-06, 'epoch': 9.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 20500/22020 [1:29:34<06:16,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9056, 'learning_rate': 1.3805631244323345e-06, 'epoch': 9.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 21000/22020 [1:31:40<04:09,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8931, 'learning_rate': 9.264305177111717e-07, 'epoch': 9.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 21500/22020 [1:33:45<02:08,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9067, 'learning_rate': 4.722979109900091e-07, 'epoch': 9.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 22000/22020 [1:35:49<00:04,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9129, 'learning_rate': 1.8165304268846506e-08, 'epoch': 9.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "100%|██████████| 22020/22020 [1:36:11<00:00,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8954883813858032, 'eval_runtime': 15.931, 'eval_samples_per_second': 122.842, 'eval_steps_per_second': 15.379, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22020/22020 [1:36:31<00:00,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5791.4635, 'train_samples_per_second': 30.405, 'train_steps_per_second': 3.802, 'train_loss': 2.1037816760542176, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22020, training_loss=2.1037816760542176, metrics={'train_runtime': 5791.4635, 'train_samples_per_second': 30.405, 'train_steps_per_second': 3.802, 'train_loss': 2.1037816760542176, 'epoch': 10.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perplexity after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245/245 [00:14<00:00, 16.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 6.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245/245 [00:14<00:00, 17.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 6.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# eval_results = trainer.evaluate()\n",
    "# print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/danendra/.cache/huggingface/datasets/csv/default-5ee63b2ab57cbf46/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "                                                   \r"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset('csv', data_files='data/quadruplet_only.csv')\n",
    "tokenized_datasets = raw_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=raw_dataset['train'].column_names\n",
    ")\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 72\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' beli barang di dengan pengiriman pake si driver ngeluh hanya dapat perak sya sich gak percaya, masa sich sya jawab gitu, lah wong sy bayar rb sameday dari toko didepok, gk tau dah. intinya ada dua driver gosend, dan setelah gue liat kejadiannya di cctv kantor.. asumsi gue itu tu komplotan driver. pas baca thread ini, ya kok yakinnya malah penjualnya yang main. wkwkwk damn, iya bro. kitanya sebagai buyer jadi rugi. dia enak, tinggal ajuin klaim hilang terus dapet dana kita. sedangkan kita, harus nunggu approval dulu dan proses klaim segala macem. sial emang. gk mau buruk sangka'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(downsampled_dataset['train'][1]['input_ids'], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' beli barang di dengan pengiriman pake si driver ngeluh hanya dapat perak sya sich gak percaya, masa sich sya jawab gitu, lah wong sy bayar rb sameday dari toko didepok, gk tau dah',\n",
       " ' intinya ada dua driver gosend, dan setelah gue liat kejadiannya di cctv kantor',\n",
       " '',\n",
       " ' asumsi gue itu tu komplotan driver',\n",
       " ' pas baca thread ini, ya kok yakinnya malah penjualnya yang main',\n",
       " ' wkwkwk damn, iya bro',\n",
       " ' kitanya sebagai buyer jadi rugi',\n",
       " ' dia enak, tinggal ajuin klaim hilang terus dapet dana kita',\n",
       " ' sedangkan kita, harus nunggu approval dulu dan proses klaim segala macem',\n",
       " ' sial emang',\n",
       " ' gk mau buruk sangka']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(downsampled_dataset['train'][1]['input_ids'], skip_special_tokens=False).split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def text_infilling(examples):\n",
    "    new_inputs = []\n",
    "    new_labels = []\n",
    "    for i in range(len(examples['input_ids'])):\n",
    "        word_ids = examples['word_ids'][i]\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = examples[\"input_ids\"][i]\n",
    "        labels = examples[\"labels\"][i]\n",
    "        new_inputs = []\n",
    "        new_labels = []\n",
    "        for word_id, masked in enumerate(mask):\n",
    "            for idx in mapping[word_id]:\n",
    "                masked_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        #dropping the same extra_id\n",
    "        input_ids = drop_duplicate_id(input_ids, input_masked+1)\n",
    "        labels = drop_duplicate_id(labels, label_masked+1)\n",
    "        new_inputs.append(input_ids)\n",
    "        new_labels.append(labels)\n",
    "    examples['labels'] = new_labels\n",
    "    examples['input_ids'] = new_inputs\n",
    "    examples['attention_mask'] = [[1 for i in range(len(examples['input_ids'][j]))] for j in range(len(examples['input_ids']))]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wwm_probability = 0.2\n",
    "\n",
    "\n",
    "# def whole_word_masking(examples):\n",
    "#     new_inputs = []\n",
    "#     new_labels = []\n",
    "#     for i in range(len(examples['input_ids'])):\n",
    "#         word_ids = examples['word_ids'][i]\n",
    "#         # Create a map between words and corresponding token indices\n",
    "#         mapping = collections.defaultdict(list)\n",
    "#         current_word_index = -1\n",
    "#         current_word = None\n",
    "#         for idx, word_id in enumerate(word_ids):\n",
    "#             if word_id is not None:\n",
    "#                 if word_id != current_word:\n",
    "#                     current_word = word_id\n",
    "#                     current_word_index += 1\n",
    "#                 mapping[current_word_index].append(idx)\n",
    "\n",
    "#         # Randomly mask words\n",
    "#         mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "#         input_ids = examples[\"input_ids\"][i]\n",
    "#         labels = examples[\"labels\"][i]\n",
    "#         input_masked = 0\n",
    "#         label_masked = 0\n",
    "#         for word_id, masked in enumerate(mask):\n",
    "#             for idx in mapping[word_id]:\n",
    "#                 #if not masking input_ids then we mask the label\n",
    "#                 if masked==0:\n",
    "#                     labels[idx] = tokenizer.convert_tokens_to_ids(f'<extra_id_{label_masked}>')\n",
    "#                 #if masking then we mask the input_ids\n",
    "#                 elif masked==1:\n",
    "#                     input_ids[idx] = tokenizer.convert_tokens_to_ids(f'<extra_id_{input_masked}>')\n",
    "#             if masked==0:\n",
    "#                 label_masked+=1\n",
    "#             elif masked==1:\n",
    "#                 input_masked+=1\n",
    "#         new_inputs.append(input_ids)\n",
    "#         new_labels.append(labels)\n",
    "#     examples['labels'] = new_labels\n",
    "#     examples['input_ids'] = new_inputs\n",
    "#     return examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
